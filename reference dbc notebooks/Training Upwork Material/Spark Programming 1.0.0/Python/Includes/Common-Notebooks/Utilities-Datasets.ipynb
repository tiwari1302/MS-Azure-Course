{"cells":[{"cell_type":"code","source":["\n%python\n# ****************************************************************************\n# Utility method to count & print the number of records in each partition.\n# ****************************************************************************\n\ndef printRecordsPerPartition(df):\n  def countInPartition(iterator): yield __builtin__.sum(1 for _ in iterator)\n  results = (df.rdd                   # Convert to an RDD\n    .mapPartitions(countInPartition)  # For each partition, count\n    .collect()                        # Return the counts to the driver\n  )\n  \n  print(\"Per-Partition Counts\")\n  i = 0\n  for result in results: \n    i = i + 1\n    print(\"#{}: {:,}\".format(i, result))\n  \n# ****************************************************************************\n# Utility to count the number of files in and size of a directory\n# ****************************************************************************\n\ndef computeFileStats(path):\n  bytes = 0\n  count = 0\n\n  files = dbutils.fs.ls(path)\n  \n  while (len(files) > 0):\n    fileInfo = files.pop(0)\n    if (fileInfo.isDir() == False):               # isDir() is a method on the fileInfo object\n      count += 1\n      bytes += fileInfo.size                      # size is a parameter on the fileInfo object\n    else:\n      files.extend(dbutils.fs.ls(fileInfo.path))  # append multiple object to files\n      \n  return (count, bytes)\n\n# ****************************************************************************\n# Utility method to cache a table with a specific name\n# ****************************************************************************\n\ndef cacheAs(df, name, level = \"MEMORY-ONLY\"):\n  from pyspark.sql.utils import AnalysisException\n  if level != \"MEMORY-ONLY\":\n    print(\"WARNING: The PySpark API currently does not allow specification of the storage level - using MEMORY-ONLY\")  \n    \n  try: spark.catalog.uncacheTable(name)\n  except AnalysisException: None\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name)\n  \n  return df\n\n\n# ****************************************************************************\n# Simplified benchmark of count()\n# ****************************************************************************\n\ndef benchmarkCount(func):\n  import time\n  start = float(time.time() * 1000)                    # Start the clock\n  df = func()\n  total = df.count()                                   # Count the records\n  duration = float(time.time() * 1000) - start         # Stop the clock\n  return (df, total, duration)\n\n# ****************************************************************************\n# Utility methods to terminate streams\n# ****************************************************************************\n\ndef getActiveStreams():\n  try:\n    return spark.streams.active\n  except:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"Unable to iterate over all active streams - using an empty set instead.\")\n    return []\n\ndef stopStream(s):\n  try:\n    print(\"Stopping the stream {}.\".format(s.name))\n    s.stop()\n    print(\"The stream {} was stopped.\".format(s.name))\n  except:\n    # In extream cases, this funtion may throw an ignorable error.\n    print(\"An [ignorable] error has occured while stoping the stream.\")\n\ndef stopAllStreams():\n  streams = getActiveStreams()\n  while len(streams) > 0:\n    stopStream(streams[0])\n    streams = getActiveStreams()\n    \n# ****************************************************************************\n# Utility method to wait until the stream is read\n# ****************************************************************************\n\ndef untilStreamIsReady(name, progressions=3):\n  import time\n  queries = list(filter(lambda query: query.name == name or query.name == name + \"_p\", getActiveStreams()))\n\n  while (len(queries) == 0 or len(queries[0].recentProgress) < progressions):\n    time.sleep(5) # Give it a couple of seconds\n    queries = list(filter(lambda query: query.name == name or query.name == name + \"_p\", getActiveStreams()))\n\n  print(\"The stream {} is active and ready.\".format(name))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2c8a281-1f7a-4759-a2ed-d2d9ec768ebb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// ****************************************************************************\n// Utility method to count & print the number of records in each partition.\n// ****************************************************************************\n\ndef printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]):Unit = {\n  // import org.apache.spark.sql.functions._\n  val results = df.rdd                                   // Convert to an RDD\n    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n    .collect()                                           // Return the counts to the driver\n\n  println(\"Per-Partition Counts\")\n  var i = 0\n  for (r <- results) {\n    i = i +1\n    println(\"#%s: %,d\".format(i,r))\n  }\n}\n\n// ****************************************************************************\n// Utility to count the number of files in and size of a directory\n// ****************************************************************************\n\ndef computeFileStats(path:String):(Long,Long) = {\n  var bytes = 0L\n  var count = 0L\n\n  import scala.collection.mutable.ArrayBuffer\n  var files=ArrayBuffer(dbutils.fs.ls(path):_ *)\n\n  while (files.isEmpty == false) {\n    val fileInfo = files.remove(0)\n    if (fileInfo.isDir == false) {\n      count += 1\n      bytes += fileInfo.size\n    } else {\n      files.append(dbutils.fs.ls(fileInfo.path):_ *)\n    }\n  }\n  (count, bytes)\n}\n\n// ****************************************************************************\n// Utility method to cache a table with a specific name\n// ****************************************************************************\n\ndef cacheAs(df:org.apache.spark.sql.DataFrame, name:String, level:org.apache.spark.storage.StorageLevel):org.apache.spark.sql.DataFrame = {\n  try spark.catalog.uncacheTable(name)\n  catch { case _: org.apache.spark.sql.AnalysisException => () }\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name, level)\n  return df\n}\n\n// ****************************************************************************\n// Simplified benchmark of count()\n// ****************************************************************************\n\ndef benchmarkCount(func:() => org.apache.spark.sql.DataFrame):(org.apache.spark.sql.DataFrame, Long, Long) = {\n  val start = System.currentTimeMillis            // Start the clock\n  val df = func()                                 // Get our lambda\n  val total = df.count()                          // Count the records\n  val duration = System.currentTimeMillis - start // Stop the clock\n  (df, total, duration)\n}\n\n// ****************************************************************************\n// Benchmarking and cache tracking tool\n// ****************************************************************************\n\ncase class JobResults[T](runtime:Long, duration:Long, cacheSize:Long, maxCacheBefore:Long, remCacheBefore:Long, maxCacheAfter:Long, remCacheAfter:Long, result:T) {\n  def printTime():Unit = {\n    if (runtime < 1000)                 println(f\"Runtime:  ${runtime}%,d ms\")\n    else if (runtime < 60 * 1000)       println(f\"Runtime:  ${runtime/1000.0}%,.2f sec\")\n    else if (runtime < 60 * 60 * 1000)  println(f\"Runtime:  ${runtime/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Runtime:  ${runtime/1000.0/60.0/60.0}%,.2f hr\")\n    \n    if (duration < 1000)                println(f\"All Jobs: ${duration}%,d ms\")\n    else if (duration < 60 * 1000)      println(f\"All Jobs: ${duration/1000.0}%,.2f sec\")\n    else if (duration < 60 * 60 * 1000) println(f\"All Jobs: ${duration/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Job Dur: ${duration/1000.0/60.0/60.0}%,.2f hr\")\n  }\n  def printCache():Unit = {\n    if (Math.abs(cacheSize) < 1024)                    println(f\"Cached:   ${cacheSize}%,d bytes\")\n    else if (Math.abs(cacheSize) < 1024 * 1024)        println(f\"Cached:   ${cacheSize/1024.0}%,.3f KB\")\n    else if (Math.abs(cacheSize) < 1024 * 1024 * 1024) println(f\"Cached:   ${cacheSize/1024.0/1024.0}%,.3f MB\")\n    else                                               println(f\"Cached:   ${cacheSize/1024.0/1024.0/1024.0}%,.3f GB\")\n    \n    println(f\"Before:   ${remCacheBefore / 1024.0 / 1024.0}%,.3f / ${maxCacheBefore / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheBefore/maxCacheBefore}%.2f%%\")\n    println(f\"After:    ${remCacheAfter / 1024.0 / 1024.0}%,.3f / ${maxCacheAfter / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheAfter/maxCacheAfter}%.2f%%\")\n  }\n  def print():Unit = {\n    printTime()\n    printCache()\n  }\n}\n\ncase class Node(driver:Boolean, executor:Boolean, address:String, maximum:Long, available:Long) {\n  def this(address:String, maximum:Long, available:Long) = this(address.contains(\"-\"), !address.contains(\"-\"), address, maximum, available)\n}\n\nclass Tracker() extends org.apache.spark.scheduler.SparkListener() {\n  \n  sc.addSparkListener(this)\n  \n  val jobStarts = scala.collection.mutable.Map[Int,Long]()\n  val jobEnds = scala.collection.mutable.Map[Int,Long]()\n  \n  def track[T](func:() => T):JobResults[T] = {\n    jobEnds.clear()\n    jobStarts.clear()\n\n    val executorsBefore = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheBefore = executorsBefore.map(_.maximum).sum\n    val remCacheBefore = executorsBefore.map(_.available).sum\n    \n    val start = System.currentTimeMillis()\n    val result = func()\n    val runtime = System.currentTimeMillis() - start\n    \n    Thread.sleep(1000) // give it a second to catch up\n\n    val executorsAfter = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheAfter = executorsAfter.map(_.maximum).sum\n    val remCacheAfter = executorsAfter.map(_.available).sum\n\n    var duration = 0L\n    \n    for ((jobId, startAt) <- jobStarts) {\n      assert(jobEnds.keySet.exists(_ == jobId), s\"A conclusion for Job ID $jobId was not found.\") \n      duration += jobEnds(jobId) - startAt\n    }\n    JobResults(runtime, duration, remCacheBefore-remCacheAfter, maxCacheBefore, remCacheBefore, maxCacheAfter, remCacheAfter, result)\n  }\n  override def onJobStart(jobStart: org.apache.spark.scheduler.SparkListenerJobStart):Unit = jobStarts.put(jobStart.jobId, jobStart.time)\n  override def onJobEnd(jobEnd: org.apache.spark.scheduler.SparkListenerJobEnd): Unit = jobEnds.put(jobEnd.jobId, jobEnd.time)\n}\n\nval tracker = new Tracker()\n\n// ****************************************************************************\n// Utility methods to terminate streams\n// ****************************************************************************\n\ndef getActiveStreams():Seq[org.apache.spark.sql.streaming.StreamingQuery] = {\n  return try {\n    spark.streams.active\n  } catch {\n    case e:Throwable => {\n      // In extream cases, this funtion may throw an ignorable error.\n      println(\"Unable to iterate over all active streams - using an empty set instead.\")\n      Seq[org.apache.spark.sql.streaming.StreamingQuery]()\n    }\n  }\n}\n\ndef stopStream(s:org.apache.spark.sql.streaming.StreamingQuery):Unit = {\n  try {\n    s.stop()\n  } catch {\n    case e:Throwable => {\n      // In extream cases, this funtion may throw an ignorable error.\n      println(s\"An [ignorable] error has occured while stoping the stream.\")\n    }\n  }\n}\n\ndef stopAllStreams():Unit = {\n  var streams = getActiveStreams()\n  while (streams.length > 0) {\n    stopStream(streams(0))\n    streams = getActiveStreams()\n  }\n}\n\n// ****************************************************************************\n// Utility method to wait until the stream is read\n// ****************************************************************************\n\ndef untilStreamIsReady(name:String, progressions:Int = 3):Unit = {\n  var queries = getActiveStreams().filter(s => s.name == name || s.name == name + \"_s\")\n  \n  while (queries.length == 0 || queries(0).recentProgress.length < progressions) {\n    Thread.sleep(5*1000) // Give it a couple of seconds\n    queries = getActiveStreams().filter(s => s.name == name || s.name == name + \"_s\")\n  }\n  println(\"The stream %s is active and ready.\".format(name))\n}\n\n//**********************************\n// CREATE THE MOUNTS\n//**********************************\n\ndef getAwsRegion():String = {\n  try {\n    import scala.io.Source\n    import scala.util.parsing.json._\n\n    val jsonString = Source.fromURL(\"http://169.254.169.254/latest/dynamic/instance-identity/document\").mkString // reports ec2 info\n    val map = JSON.parseFull(jsonString).getOrElse(null).asInstanceOf[Map[Any,Any]]\n    map.getOrElse(\"region\", null).asInstanceOf[String]\n\n  } catch {\n    // We will use this later to know if we are Amazon vs Azure\n    case _: java.io.FileNotFoundException => null\n  }\n}\n\ndef getAzureRegion():String = {\n  import com.databricks.backend.common.util.Project\n  import com.databricks.conf.trusted.ProjectConf\n  import com.databricks.backend.daemon.driver.DriverConf\n\n  new DriverConf(ProjectConf.loadLocalConfig(Project.Driver)).region\n}\n\n// These keys are read-only so they're okay to have here\nval awsAccessKey = \"AKIAJBRYNXGHORDHZB4A\"\nval awsSecretKey = \"a0BzE1bSegfydr3%2FGE3LSPM6uIV5A4hOUfpH8aFF\"\nval awsAuth = s\"${awsAccessKey}:${awsSecretKey}\"\n\ndef getAwsMapping(region:String):(String,Map[String,String]) = {\n\n  val MAPPINGS = Map(\n    \"ap-northeast-1\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-northeast-1/common\", Map[String,String]()),\n    \"ap-northeast-2\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-northeast-2/common\", Map[String,String]()),\n    \"ap-south-1\"     -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-south-1/common\", Map[String,String]()),\n    \"ap-southeast-1\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-southeast-1/common\", Map[String,String]()),\n    \"ap-southeast-2\" -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ap-southeast-2/common\", Map[String,String]()),\n    \"ca-central-1\"   -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-ca-central-1/common\", Map[String,String]()),\n    \"eu-central-1\"   -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-central-1/common\", Map[String,String]()),\n    \"eu-west-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-1/common\", Map[String,String]()),\n    \"eu-west-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-2/common\", Map[String,String]()),\n\n    // eu-west-3 in Paris isn't supported by Databricks yet - not supported by the current version of the AWS library\n    // \"eu-west-3\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-west-3/common\", Map[String,String]()),\n    \n    // Use Frankfurt in EU-Central-1 instead\n    \"eu-west-3\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-eu-central-1/common\", Map[String,String]()),\n    \n    \"sa-east-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-sa-east-1/common\", Map[String,String]()),\n    \"us-east-1\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-us-east-1/common\", Map[String,String]()),\n    \"us-east-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training-us-east-2/common\", Map[String,String]()),\n    \"us-west-2\"      -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training/common\", Map[String,String]()),\n    \"_default\"       -> (s\"s3a://${awsAccessKey}:${awsSecretKey}@databricks-corp-training/common\", Map[String,String]())\n  )\n\n  MAPPINGS.getOrElse(region, MAPPINGS(\"_default\"))\n}\n\ndef getAzureMapping(region:String):(String,Map[String,String]) = {\n\n  var MAPPINGS = Map(\n    \"australiacentral\"    -> (\"dbtrainaustraliasoutheas\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"australiacentral2\"   -> (\"dbtrainaustraliasoutheas\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"australiaeast\"       -> (\"dbtrainaustraliaeast\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=FM6dy59nmw3f4cfN%2BvB1cJXVIVz5069zHmrda5gZGtU%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"australiasoutheast\"  -> (\"dbtrainaustraliasoutheas\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=br8%2B5q2ZI9osspeuPtd3haaXngnuWPnZaHKFoLmr370%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"canadacentral\"       -> (\"dbtraincanadacentral\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=dwAT0CusWjvkzcKIukVnmFPTmi4JKlHuGh9GEx3OmXI%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"canadaeast\"          -> (\"dbtraincanadaeast\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=SYmfKBkbjX7uNDnbSNZzxeoj%2B47PPa8rnxIuPjxbmgk%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"centralindia\"        -> (\"dbtraincentralindia\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=afrYm3P5%2BB4gMg%2BKeNZf9uvUQ8Apc3T%2Bi91fo/WOZ7E%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"centralus\"           -> (\"dbtraincentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=As9fvIlVMohuIV8BjlBVAKPv3C/xzMRYR1JAOB%2Bbq%2BQ%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"eastasia\"            -> (\"dbtraineastasia\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=sK7g5pki8bE88gEEsrh02VGnm9UDlm55zTfjZ5YXVMc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"eastus\"              -> (\"dbtraineastus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=tlw5PMp1DMeyyBGTgZwTbA0IJjEm83TcCAu08jCnZUo%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"eastus2\"             -> (\"dbtraineastus2\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=Y6nGRjkVj6DnX5xWfevI6%2BUtt9dH/tKPNYxk3CNCb5A%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"japaneast\"           -> (\"dbtrainjapaneast\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=q6r9MS/PC9KLZ3SMFVYO94%2BfM5lDbAyVsIsbBKEnW6Y%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"japanwest\"           -> (\"dbtrainjapanwest\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=M7ic7/jOsg/oiaXfo8301Q3pt9OyTMYLO8wZ4q8bko8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"northcentralus\"      -> (\"dbtrainnorthcentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=GTLU0g3pajgz4dpGUhOpJHBk3CcbCMkKT8wxlhLDFf8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"northcentralus\"      -> (\"dbtrainnorthcentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=GTLU0g3pajgz4dpGUhOpJHBk3CcbCMkKT8wxlhLDFf8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"northeurope\"         -> (\"dbtrainnortheurope\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=35yfsQBGeddr%2BcruYlQfSasXdGqJT3KrjiirN/a3dM8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"southcentralus\"      -> (\"dbtrainsouthcentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=3cnVg/lzWMx5XGz%2BU4wwUqYHU5abJdmfMdWUh874Grc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"southcentralus\"      -> (\"dbtrainsouthcentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=3cnVg/lzWMx5XGz%2BU4wwUqYHU5abJdmfMdWUh874Grc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"southindia\"          -> (\"dbtrainsouthindia\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=0X0Ha9nFBq8qkXEO0%2BXd%2B2IwPpCGZrS97U4NrYctEC4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"southeastasia\"       -> (\"dbtrainsoutheastasia\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=H7Dxi1yqU776htlJHbXd9pdnI35NrFFsPVA50yRC9U0%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"uksouth\"             -> (\"dbtrainuksouth\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=SPAI6IZXmm%2By/WMSiiFVxp1nJWzKjbBxNc5JHUz1d1g%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"ukwest\"              -> (\"dbtrainukwest\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=olF4rjQ7V41NqWRoK36jZUqzDBz3EsyC6Zgw0QWo0A8%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"westcentralus\"       -> (\"dbtrainwestcentralus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=UP0uTNZKMCG17IJgJURmL9Fttj2ujegj%2BrFN%2B0OszUE%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"westeurope\"          -> (\"dbtrainwesteurope\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=csG7jGsNFTwCArDlsaEcU4ZUJFNLgr//VZl%2BhdSgEuU%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"westindia\"           -> (\"dbtrainwestindia\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=fI6PNZ7YvDGKjArs1Et2rAM2zgg6r/bsKEjnzQxgGfA%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"westus\"              -> (\"dbtrainwestus\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=%2B1XZDXbZqnL8tOVsmRtWTH/vbDAKzih5ThvFSZMa3Tc%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"westus2\"             -> (\"dbtrainwestus2\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=DD%2BO%2BeIZ35MO8fnh/fk4aqwbne3MAJ9xh9aCIU/HiD4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\"),\n    \"_default\"            -> (\"dbtrainwestus2\",\n                              \"?ss=b&sp=rl&sv=2018-03-28&st=2018-04-01T00%3A00%3A00Z&sig=DD%2BO%2BeIZ35MO8fnh/fk4aqwbne3MAJ9xh9aCIU/HiD4%3D&srt=sco&se=2023-04-01T00%3A00%3A00Z\")\n  )\n\n  val (account: String, sasKey: String) = MAPPINGS.getOrElse(region, MAPPINGS(\"_default\"))\n\n  val blob = \"training\"\n  val source = s\"wasbs://$blob@$account.blob.core.windows.net/\"\n  val configMap = Map(\n    s\"fs.azure.sas.$blob.$account.blob.core.windows.net\" -> sasKey\n  )\n\n  (source, configMap)\n}\n\ndef mountFailed(msg:String): Unit = {\n  println(msg)\n}\n\ndef retryMount(source: String, mountPoint: String): Unit = {\n  try { \n    // Mount with IAM roles instead of keys for PVC\n    dbutils.fs.mount(source, mountPoint)\n  } catch {\n    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n  }\n}\n\ndef mount(source: String, extraConfigs:Map[String,String], mountPoint: String): Unit = {\n  try {\n    dbutils.fs.mount(source, mountPoint, extraConfigs=extraConfigs)\n  } catch {\n    case ioe: java.lang.IllegalArgumentException => retryMount(source, mountPoint)\n    case e: Exception => mountFailed(s\"*** ERROR: Unable to mount $mountPoint: ${e.getMessage}\")\n  }\n}\n\ndef autoMount(fix:Boolean = false, failFast:Boolean = false, mountDir:String = \"/mnt/training\"): String = {\n  var awsRegion = getAwsRegion()\n\n  val (source, extraConfigs) = if (awsRegion != null)  {\n    spark.conf.set(\"com.databricks.training.region.name\", awsRegion)\n    getAwsMapping(awsRegion)\n\n  } else {\n    val azureRegion = getAzureRegion()\n    spark.conf.set(\"com.databricks.training.region.name\", azureRegion)\n    initAzureDataSource(azureRegion)\n  }\n  \n  val resultMsg = mountSource(fix, failFast, mountDir, source, extraConfigs)\n  resultMsg\n}\n\ndef initAzureDataSource(azureRegion:String):(String,Map[String,String]) = {\n  val mapping = getAzureMapping(azureRegion)\n  val (source, config) = mapping\n  val (sasEntity, sasToken) = config.head\n\n  val datasource = \"%s\\t%s\\t%s\".format(source, sasEntity, sasToken)\n  spark.conf.set(\"com.databricks.training.azure.datasource\", datasource)\n\n  return mapping\n}\n\ndef mountSource(fix:Boolean, failFast:Boolean, mountDir:String, source:String, extraConfigs:Map[String,String]): String = {\n  val mntSource = source.replace(awsAuth+\"@\", \"\")\n\n  if (dbutils.fs.mounts().map(_.mountPoint).contains(mountDir)) {\n    val mount = dbutils.fs.mounts().filter(_.mountPoint == mountDir).head\n    if (mount.source == mntSource) {\n      return s\"\"\"Datasets are already mounted to <b>$mountDir</b> from <b>$mntSource</b>\"\"\"\n      \n    } else if (failFast) {\n      throw new IllegalStateException(s\"Expected $mntSource but found ${mount.source}\")\n      \n    } else if (fix) {\n      println(s\"Unmounting existing datasets ($mountDir from $mntSource)\")\n      dbutils.fs.unmount(mountDir)\n      mountSource(fix, failFast, mountDir, source, extraConfigs)\n\n    } else {\n      return s\"\"\"<b style=\"color:red\">Invalid Mounts!</b></br>\n                      <ul>\n                      <li>The training datasets you are using are from an unexpected source</li>\n                      <li>Expected <b>$mntSource</b> but found <b>${mount.source}</b></li>\n                      <li>Failure to address this issue may result in significant performance degradation. To address this issue:</li>\n                      <ol>\n                        <li>Insert a new cell after this one</li>\n                        <li>In that new cell, run the command <code style=\"color:blue; font-weight:bold\">%scala fixMounts()</code></li>\n                        <li>Verify that the problem has been resolved.</li>\n                      </ol>\"\"\"\n    }\n  } else {\n    println(s\"\"\"Mounting datasets to $mountDir from $mntSource\"\"\")\n    mount(source, extraConfigs, mountDir)\n    return s\"\"\"Mounted datasets to <b>$mountDir</b> from <b>$mntSource<b>\"\"\"\n  }\n}\n\ndef fixMounts(): Unit = {\n  autoMount(true)\n}\n\nval resultMsg = autoMount(true)\n\ndisplayHTML(\"Datasets mounted and student environment set up\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf94bd5b-e4de-4956-9723-06e8fe37921f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Utilities-Datasets","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744914489}},"nbformat":4,"nbformat_minor":0}
