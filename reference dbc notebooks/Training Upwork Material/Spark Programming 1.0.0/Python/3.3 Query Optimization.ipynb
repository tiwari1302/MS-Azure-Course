{"cells":[{"cell_type":"markdown","source":["d-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 400px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4062422a-235c-4c3c-921e-64889e34e9ad"}}},{"cell_type":"markdown","source":["# Query Optimization\n1. Logical optimizations\n1. Predicate pushdown\n1. No predicate pushdown\n\n##### Methods\n- DataFrame (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html\" target=\"_blank\">Scala</a>): `explain`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53151753-e59b-4676-bd90-d564ebd0acc9"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92c7ecc1-8506-4f70-bc14-ebfe83ac7f72"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.parquet(eventsPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e98dbef8-bf3c-48db-8a47-0cce5c6355f4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Logical Optimization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04bb21b7-4d60-4ba7-80f7-208c7778b7b3"}}},{"cell_type":"markdown","source":["#### `explain(..)`\n\nPrints the plans (logical and physical), optionally formatted by a given explain mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abf23a97-84b9-47aa-b8ad-42c07705f485"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nlimitEventsDF = (df\n  .filter(col(\"event_name\") != \"reviews\")\n  .filter(col(\"event_name\") != \"checkout\")\n  .filter(col(\"event_name\") != \"register\")\n  .filter(col(\"event_name\") != \"email_coupon\")\n  .filter(col(\"event_name\") != \"cc_info\")\n  .filter(col(\"event_name\") != \"delivery\")\n  .filter(col(\"event_name\") != \"shipping_info\")\n  .filter(col(\"event_name\") != \"press\")\n)\n\nlimitEventsDF.count()\n\nlimitEventsDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ceb5255-e3f3-4bfb-a471-5a53001680d8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["betterDF = (df.filter( \n  (col(\"event_name\").isNotNull()) &\n  (col(\"event_name\") != \"reviews\") & \n  (col(\"event_name\") != \"checkout\") & \n  (col(\"event_name\") != \"register\") & \n  (col(\"event_name\") != \"email_coupon\") & \n  (col(\"event_name\") != \"cc_info\") & \n  (col(\"event_name\") != \"delivery\") & \n  (col(\"event_name\") != \"shipping_info\") & \n  (col(\"event_name\") != \"press\")\n))\n\nbetterDF.count()\n\nbetterDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc0eeac6-24c1-4748-ba36-9b9988aad176"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stupidDF = (df\n  .filter(col(\"event_name\") != \"finalize\")\n  .filter(col(\"event_name\") != \"finalize\")\n  .filter(col(\"event_name\") != \"finalize\")\n  .filter(col(\"event_name\") != \"finalize\")\n  .filter(col(\"event_name\") != \"finalize\")\n)\n\nstupidDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4946895-846f-4670-a912-b1c9de40073c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Predicate Pushdown\n\nHere is example with JDBC where predicate pushdown takes place"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dca92fde-a153-42b3-8ae4-daf1a7201696"}}},{"cell_type":"code","source":["%scala\n// Ensure that the driver class is loaded\nClass.forName(\"org.postgresql.Driver\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ff28330-4438-42a3-94f0-a68ea1bd1b1a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n\n# Username and Password w/read-only rights\nconnProperties = {\n  \"user\" : \"training\",\n  \"password\" : \"training\"\n}\n\nppDF = (spark.read.jdbc(\n    url=jdbcURL,                  # the JDBC URL\n    table=\"training.people_1m\",   # the name of the table\n    column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n    lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n    upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n    numPartitions=8,              # the number of partitions/connections\n    properties=connProperties     # the connection properties\n  )\n  .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd71497b-c0e5-4a02-a7ec-763b0ab646d6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ppDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a821d37-b02d-4ceb-a31b-7d55deef818b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note the lack of a **Filter** and the presence of a **PushedFilters** in the **Scan**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34c207f0-cd83-4aa0-9340-a2b603810da7"}}},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) No Predicate Pushdown\n \nThis will make a little more sense if we **compare it to examples** that don't push down the filter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bde41b62-5ea9-40f6-bc8a-03e8816aff6b"}}},{"cell_type":"markdown","source":["Caching the data before filtering eliminates the possibility for the predicate push down"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3eb3864-2f87-4abc-8309-2468c6248c58"}}},{"cell_type":"code","source":["cachedDF = (spark.read.jdbc(\n    url=jdbcURL,\n    table=\"training.people_1m\",\n    column=\"id\",\n    lowerBound=1,\n    upperBound=1000000,\n    numPartitions=8,\n    properties=connProperties\n  ))\n\ncachedDF.cache().count()\n\nfilteredDF = cachedDF.filter(col(\"gender\") == \"M\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2373ac3e-876a-4e65-a0b7-01e08093cc6f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In addition to the **Scan** (the JDBC read) we saw in the previous example, here we also see the **InMemoryTableScan** followed by a **Filter** in the explain plan.\n\nThis means Spark had to filter ALL the data from RAM instead of in the Database."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be0af3e9-20e3-45b3-acef-cab1f573c921"}}},{"cell_type":"code","source":["filteredDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e516919-3892-4e6e-9d47-37fb20bc3e77"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here is another example using CSV where predicate pushdown does **not** place"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5554dd9-3a15-466c-9fe3-e93421c7c7db"}}},{"cell_type":"code","source":["csvDF = (spark.read\n  .option(\"header\", \"true\")\n  .option(\"sep\", \"\\t\")\n  .option(\"inferSchema\", \"true\")\n  .csv(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\")\n  .filter(col(\"site\") == \"desktop\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac217a38-0593-40c7-8773-c86e139b287d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note the presence of a **Filter** and **PushedFilters** in the **FileScan csv**\n\nAgain, we see **PushedFilters** because Spark is *trying* to push down to the CSV file.\n\nHowever, this does not work here, and thus we see, like in the last example, we have a **Filter** after the **FileScan**, actually an **InMemoryFileIndex**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7092a7e7-c454-40ca-8269-2f8491007797"}}},{"cell_type":"code","source":["csvDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"862a37cc-b88a-4a9d-980b-7a24eed9447b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdba58b8-5df8-4705-85d0-a3c6a4581745"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06ee4ab8-d83b-4b61-8fa5-8385e58d9f70"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.3 Query Optimization","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744914253}},"nbformat":4,"nbformat_minor":0}
