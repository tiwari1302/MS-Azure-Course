{"cells":[{"cell_type":"markdown","source":["d\n# Delta Lake\n\n1. Create a Delta Table\n1. Understand the Transaction Log\n1. Read data from your Delta Table\n1. Update data in your Delta Table\n1. Access previous versions of table using time travel\n1. Vacuum\n\n##### Documentation\n- <a href=\"https://docs.delta.io/latest/quick-start.html#create-a-table\" target=\"_blank\">Delta Table</a> \n- <a href=\"https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html\" target=\"_blank\">Transaction Log</a> \n- <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\" target=\"_blank\">Time Travel</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a47cf30e-e84c-4d51-aff2-247611af4446"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6269e530-f41e-4d3f-9910-ce1b62c1b4a2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Create a Delta Table\nLet's use the BedBricks events dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6767c484-5b0e-4744-964c-55167f79706e"}}},{"cell_type":"code","source":["eventsDF = spark.read.parquet(eventsPath)\ndisplay(eventsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcbd15d7-4b3f-4dc6-930b-0cee39a8f805"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Convert data to a Delta table using the schema provided by the DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fdef696-3ac8-4d50-88cc-5cc30b411cbe"}}},{"cell_type":"code","source":["deltaPath = workingDir + \"/delta-events\"\neventsDF.write.format(\"delta\").mode(\"overwrite\").save(deltaPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42c3f40c-9b50-4aa2-8166-08ac51fb9ba8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can also create a Delta table in the metastore"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"664d96ea-57d3-4e1e-ab7c-5e44ae63150e"}}},{"cell_type":"code","source":["eventsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_events\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a5cba22-a575-4aa1-a00a-67951149f5fc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Delta supports partitioning your data using unique values in a specified column.\n\nLet's partition by state. This gives us a point of quick comparison between different parts of the US."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97f149c6-e1ec-4591-a89f-f3bc5d53ea3a"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nstateEventsDF = eventsDF.withColumn(\"state\", col(\"geo.state\"))\n\nstateEventsDF.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").option(\"overwriteSchema\", \"true\").save(deltaPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5dc8cf-114a-47e9-8bf8-b40ef46c7b41"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Understand the Transaction Log\nWe can see how Delta stores the different state partitions in separate files.\n\nAdditionally, we can also see a directory called `_delta_log`, its transaction log.\n\nWhen a Delta Lake table is created, its transaction log is automatically created in the `_delta_log` subdirectory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a29ef978-36fa-46ab-9fab-fbbce8b965eb"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a500fa3-3ff9-4874-9664-54e5914f6b21"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nWhen changes are made to that table, these changes are recorded as ordered, atomic commits in the transaction log.\n\nEach commit is written out as a JSON file, starting with 000000.json.\n\nAdditional changes to the table generate subsequent JSON files in ascending numerical order.\n\n<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://user-images.githubusercontent.com/20408077/87174138-609fe600-c29c-11ea-90cc-84df0c1357f1.png\" width=\"500\"/>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3acb0b5a-ff6a-4700-bc3e-f75c6565ad8d"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaPath + \"/_delta_log/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3352aa3f-1683-487f-ba2d-bb184c4214b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, let's take a look at a Transaction Log File.\n\n\nThe <a href=\"https://docs.databricks.com/delta/delta-utility.html\" target=\"_blank\">four columns</a> each represent a different part of the very first commit to the Delta Table, creating the table.\n- The `add` column has statistics about the DataFrame as a whole and individual columns.\n- The `commitInfo` column has useful information about what the operation was (WRITE or READ) and who executed the operation.\n- The `metaData` column contains information about the column schema.\n- The `protocol` version contains information about the minimum Delta version necessary to either write or read to this Delta Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e64c598-9ef5-4dbc-9464-ca765bbced87"}}},{"cell_type":"code","source":["display(spark.read.json(deltaPath + \"/_delta_log/00000000000000000000.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f36984a-7021-4bcd-9d23-a614adc140aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One key difference between these two transaction logs is the size of the JSON file, this file has 206 rows compared to the previous 7.\n\nTo understand why, let's take a look at the `commitInfo` column. We can see that in the `operationParameters` section, `partitionBy` has been filled in by the `state` column. Furthermore, if we look at the add section on row 3, we can see that a new section called `partitionValues` has appeared. As we saw above, Delta stores partitions separately in memory, however, it stores information about these partitions in the same transaction log file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76eb8858-0606-4957-b3d0-80cf92bc0caf"}}},{"cell_type":"code","source":["display(spark.read.json(deltaPath + \"/_delta_log/00000000000000000001.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"180b8671-7799-4196-ac80-9dea0a197a40"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, let's take a look at the files inside one of the state partitions. The files inside corresponds to the partition commit (file 01) in the _delta_log directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e0a64d6-1100-42d8-8c59-fe0c4d388313"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4704d54-33cd-479a-ac87-666412454659"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from your Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43d5ccee-43d0-458c-a8c2-f3823742be12"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(deltaPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2a22226-8981-4f2f-a445-a9a50eb0a62d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Update your Delta Table\n\nLet's filter for rows where the event takes place on a mobile device."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"092e757a-1eee-40c1-84e8-f21bdbc043ac"}}},{"cell_type":"code","source":["df_update = stateEventsDF.filter(col(\"device\").isin([\"Android\", \"iOS\"]))\ndisplay(df_update)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2ebccbd-2135-4046-9613-5da5211cf581"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_update.write.format(\"delta\").mode(\"overwrite\").save(deltaPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c75e0f37-82c0-4fba-ae73-d85a694e5c32"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(deltaPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c458ba4e-cc98-4234-9507-448501bb2b57"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the files in the California partition post-update. Remember, the different files in this directory are snapshots of your DataFrame corresponding to different commits."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3379fadd-3275-43ee-a576-c8154ff716c2"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e36e2b5c-8a27-4951-9c6f-8785b4fa44c0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Access previous versions of table using Time  Travel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c543e49f-3412-408e-b137-e785ffbb74ee"}}},{"cell_type":"markdown","source":["Oops, it turns out we actually we need the entire dataset! You can access a previous version of your Delta Table using Time Travel. Use the following two cells to access your version history. Delta Lake will keep a 30 day version history by default, but if necessary, Delta can store a version history for longer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c817cc4-270f-499a-bd12-b2074d65f7e0"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS train_delta\")\nspark.sql(f\"CREATE TABLE train_delta USING DELTA LOCATION '{deltaPath}'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ed6f960-6a53-46cf-93a0-b38f1c3f1ca2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY train_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b1de935-d933-458a-b8ce-d170e065ee4b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Using the `versionAsOf` option allows you to easily access previous versions of our Delta Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a73f0693-bacc-45d2-adbd-d2a9fde7e4d4"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c750d064-d6de-422c-b638-3366c7d138c8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can also access older versions using a timestamp.\n\nReplace the timestamp string with the information from your version history. Note that you can use a date without the time information if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"341e9216-e553-42c8-ad66-7908d80072a9"}}},{"cell_type":"code","source":["# TODO\ntimeStampString = <FILL_IN>\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", timeStampString).load(deltaPath)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ba903c2-9b16-4ae9-801a-60ea0f98fd10"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Vacuum\n\nNow that we're happy with our Delta Table, we can clean up our directory using `VACUUM`. Vacuum accepts a retention period in hours as an input."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b91d4551-da3c-46b7-bae4-0fa8bfe1caa0"}}},{"cell_type":"markdown","source":["It looks like our code doesn't run! By default, to prevent accidentally vacuuming recent commits, Delta Lake will not let users vacuum a period under 7 days or 168 hours. Once vacuumed, you cannot return to a prior commit through time travel, only your most recent Delta Table will be saved."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cdd517b-34e3-4bab-bbd4-81b9715e4cbc"}}},{"cell_type":"code","source":["# from delta.tables import *\n\n# deltaTable = DeltaTable.forPath(spark, deltaPath)\n# deltaTable.vacuum(0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6355d0e-2698-4629-b62e-2ae941b50ff0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can workaround this by setting a spark configuration that will bypass the default retention period check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4c520cc-7c1b-4f23-b99e-488ddb046677"}}},{"cell_type":"code","source":["from delta.tables import *\n\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.vacuum(0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daab398d-7d74-42aa-a643-2d3fe78c3064"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at our Delta Table files now. After vacuuming, the directory only holds the partition of our most recent Delta Table commit."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64eb62f9-fa39-4caf-b84b-5d7a0179a23d"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e9c4398-496b-4471-acda-02042bb295e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Since vacuuming deletes files referenced by the Delta Table, we can no longer access past versions. The code below should throw an error."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0e8576d-8181-4efe-b10d-ef89ab98ce5e"}}},{"cell_type":"code","source":["# df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\n# display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ad2d20-6020-4b0e-a0ae-1e2e51f4b6c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9423c497-0272-4d0a-9592-16ea0f6f033a"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dbb5e96-6ba0-4df0-87de-96f83fbd0f87"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"4.5 Delta Lake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744914574}},"nbformat":4,"nbformat_minor":0}
