{"cells":[{"cell_type":"markdown","source":["# Advertising Technology Sample Notebook (Part 1)\nThe purpose of this notebook is to provide example code to make sense of advertising-based web logs.  This notebook does the following:\n* Setup the connection to your S3 bucket to access the web logs\n* Create an external table against these web logs including the use of regular expression to parse the logs\n* Identity Country (ISO-3166-1 Three Letter ISO Country Codes) based on IP address by calling a REST Web service API\n* Identify Browser and OS information based on the User Agent string within the web logs using the user-agents PyPi package.\n* Convert the Apache web logs date information, create a userid, and join back to the Browser and OS information"],"metadata":{"application/vnd.databricks.v1+cell":{"title":null,"showTitle":false,"inputWidgets":{},"nuid":"22df1ab5-0064-4a20-9736-8bf30e8ca3b5"}}},{"cell_type":"markdown","source":["## Create External Table\n* Create an external table against the Ubar cars dataset\n* Instead of writing ETL logic to do this, our table definition handles this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":null,"showTitle":false,"inputWidgets":{},"nuid":"12d8b648-5443-48ab-9102-684c44466617"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/tables/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29ac51e6-04b0-4cfd-826a-bd2a83a4e172"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/Aggregated_Report_2018_03_25-d4a14.csv","Aggregated_Report_2018_03_25-d4a14.csv",55],["dbfs:/FileStore/tables/ApartmentMaintenance.json","ApartmentMaintenance.json",733358],["dbfs:/FileStore/tables/Apartment_Maintenance__1_-17a3c.csv","Apartment_Maintenance__1_-17a3c.csv",548837],["dbfs:/FileStore/tables/Apartment__1_2_json-b3c24.txt","Apartment__1_2_json-b3c24.txt",421697],["dbfs:/FileStore/tables/Apartment__1__2-d398f.csv","Apartment__1__2-d398f.csv",279449],["dbfs:/FileStore/tables/Building.json","Building.json",193401],["dbfs:/FileStore/tables/Building_Mainenance.json","Building_Mainenance.json",717938],["dbfs:/FileStore/tables/Building_Maintenance__1_-c86c7.csv","Building_Maintenance__1_-c86c7.csv",562035],["dbfs:/FileStore/tables/Building__1_-108aa.csv","Building__1_-108aa.csv",67170],["dbfs:/FileStore/tables/Contractor_Table.json","Contractor_Table.json",180883],["dbfs:/FileStore/tables/Contractor_Table__1_-6f2cb.csv","Contractor_Table__1_-6f2cb.csv",62970],["dbfs:/FileStore/tables/Cozy_Player_1000.csv","Cozy_Player_1000.csv",709920],["dbfs:/FileStore/tables/Cozy_Tx_1000.csv","Cozy_Tx_1000.csv",343078],["dbfs:/FileStore/tables/Daily_Player_2018_03_27-91b1f.csv","Daily_Player_2018_03_27-91b1f.csv",398416],["dbfs:/FileStore/tables/Daily_Player_2018_03_27___Copy-e7327.csv","Daily_Player_2018_03_27___Copy-e7327.csv",398416],["dbfs:/FileStore/tables/FileStore/","FileStore/",0],["dbfs:/FileStore/tables/Landlord.json","Landlord.json",219227],["dbfs:/FileStore/tables/Landlord__1_-dcd85.csv","Landlord__1_-dcd85.csv",73346],["dbfs:/FileStore/tables/Lease.json","Lease.json",127177],["dbfs:/FileStore/tables/Lease__1_-547f4.csv","Lease__1_-547f4.csv",36211],["dbfs:/FileStore/tables/LendingClub_loan_data.csv","LendingClub_loan_data.csv",441771600],["dbfs:/FileStore/tables/Payment.json","Payment.json",133683],["dbfs:/FileStore/tables/Payment__1_-a06e0.csv","Payment__1_-a06e0.csv",33726],["dbfs:/FileStore/tables/Rent__1_-664ec.json","Rent__1_-664ec.json",151643],["dbfs:/FileStore/tables/Rent__1_-743d8.csv","Rent__1_-743d8.csv",38691],["dbfs:/FileStore/tables/Report_2018_03_25-b9f55.csv","Report_2018_03_25-b9f55.csv",55],["dbfs:/FileStore/tables/Report_2018_03_26-07469.csv","Report_2018_03_26-07469.csv",55],["dbfs:/FileStore/tables/Tenant_1_-5ea45.csv","Tenant_1_-5ea45.csv",85930],["dbfs:/FileStore/tables/Tenant__1_-f28c6.json","Tenant__1_-f28c6.json",215879],["dbfs:/FileStore/tables/Test.csv","Test.csv",7745711],["dbfs:/FileStore/tables/Training.csv","Training.csv",350751],["dbfs:/FileStore/tables/Users_Report_2018_03_25-2a093.csv","Users_Report_2018_03_25-2a093.csv",55],["dbfs:/FileStore/tables/WA_Fn_UseC__Telco_Customer_Churn-89c80.csv","WA_Fn_UseC__Telco_Customer_Churn-89c80.csv",977501],["dbfs:/FileStore/tables/births_train_csv-dc6be.gz","births_train_csv-dc6be.gz",931988],["dbfs:/FileStore/tables/creditcard.csv","creditcard.csv",150828752],["dbfs:/FileStore/tables/hack_data.csv","hack_data.csv",14268],["dbfs:/FileStore/tables/ham","ham",355018],["dbfs:/FileStore/tables/kc_house_data.csv","kc_house_data.csv",2515206],["dbfs:/FileStore/tables/spam","spam",105420],["dbfs:/FileStore/tables/userSCV_matching.csv","userSCV_matching.csv",2328],["dbfs:/FileStore/tables/user_criteria1.csv","user_criteria1.csv",609],["dbfs:/FileStore/tables/user_criteria2.csv","user_criteria2.csv",394],["dbfs:/FileStore/tables/user_criteria3.csv","user_criteria3.csv",400],["dbfs:/FileStore/tables/user_criteria4.csv","user_criteria4.csv",408],["dbfs:/FileStore/tables/users_load.csv","users_load.csv",109006],["dbfs:/FileStore/tables/users_load1_1.csv","users_load1_1.csv",2353]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/Aggregated_Report_2018_03_25-d4a14.csv</td><td>Aggregated_Report_2018_03_25-d4a14.csv</td><td>55</td></tr><tr><td>dbfs:/FileStore/tables/ApartmentMaintenance.json</td><td>ApartmentMaintenance.json</td><td>733358</td></tr><tr><td>dbfs:/FileStore/tables/Apartment_Maintenance__1_-17a3c.csv</td><td>Apartment_Maintenance__1_-17a3c.csv</td><td>548837</td></tr><tr><td>dbfs:/FileStore/tables/Apartment__1_2_json-b3c24.txt</td><td>Apartment__1_2_json-b3c24.txt</td><td>421697</td></tr><tr><td>dbfs:/FileStore/tables/Apartment__1__2-d398f.csv</td><td>Apartment__1__2-d398f.csv</td><td>279449</td></tr><tr><td>dbfs:/FileStore/tables/Building.json</td><td>Building.json</td><td>193401</td></tr><tr><td>dbfs:/FileStore/tables/Building_Mainenance.json</td><td>Building_Mainenance.json</td><td>717938</td></tr><tr><td>dbfs:/FileStore/tables/Building_Maintenance__1_-c86c7.csv</td><td>Building_Maintenance__1_-c86c7.csv</td><td>562035</td></tr><tr><td>dbfs:/FileStore/tables/Building__1_-108aa.csv</td><td>Building__1_-108aa.csv</td><td>67170</td></tr><tr><td>dbfs:/FileStore/tables/Contractor_Table.json</td><td>Contractor_Table.json</td><td>180883</td></tr><tr><td>dbfs:/FileStore/tables/Contractor_Table__1_-6f2cb.csv</td><td>Contractor_Table__1_-6f2cb.csv</td><td>62970</td></tr><tr><td>dbfs:/FileStore/tables/Cozy_Player_1000.csv</td><td>Cozy_Player_1000.csv</td><td>709920</td></tr><tr><td>dbfs:/FileStore/tables/Cozy_Tx_1000.csv</td><td>Cozy_Tx_1000.csv</td><td>343078</td></tr><tr><td>dbfs:/FileStore/tables/Daily_Player_2018_03_27-91b1f.csv</td><td>Daily_Player_2018_03_27-91b1f.csv</td><td>398416</td></tr><tr><td>dbfs:/FileStore/tables/Daily_Player_2018_03_27___Copy-e7327.csv</td><td>Daily_Player_2018_03_27___Copy-e7327.csv</td><td>398416</td></tr><tr><td>dbfs:/FileStore/tables/FileStore/</td><td>FileStore/</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/Landlord.json</td><td>Landlord.json</td><td>219227</td></tr><tr><td>dbfs:/FileStore/tables/Landlord__1_-dcd85.csv</td><td>Landlord__1_-dcd85.csv</td><td>73346</td></tr><tr><td>dbfs:/FileStore/tables/Lease.json</td><td>Lease.json</td><td>127177</td></tr><tr><td>dbfs:/FileStore/tables/Lease__1_-547f4.csv</td><td>Lease__1_-547f4.csv</td><td>36211</td></tr><tr><td>dbfs:/FileStore/tables/LendingClub_loan_data.csv</td><td>LendingClub_loan_data.csv</td><td>441771600</td></tr><tr><td>dbfs:/FileStore/tables/Payment.json</td><td>Payment.json</td><td>133683</td></tr><tr><td>dbfs:/FileStore/tables/Payment__1_-a06e0.csv</td><td>Payment__1_-a06e0.csv</td><td>33726</td></tr><tr><td>dbfs:/FileStore/tables/Rent__1_-664ec.json</td><td>Rent__1_-664ec.json</td><td>151643</td></tr><tr><td>dbfs:/FileStore/tables/Rent__1_-743d8.csv</td><td>Rent__1_-743d8.csv</td><td>38691</td></tr><tr><td>dbfs:/FileStore/tables/Report_2018_03_25-b9f55.csv</td><td>Report_2018_03_25-b9f55.csv</td><td>55</td></tr><tr><td>dbfs:/FileStore/tables/Report_2018_03_26-07469.csv</td><td>Report_2018_03_26-07469.csv</td><td>55</td></tr><tr><td>dbfs:/FileStore/tables/Tenant_1_-5ea45.csv</td><td>Tenant_1_-5ea45.csv</td><td>85930</td></tr><tr><td>dbfs:/FileStore/tables/Tenant__1_-f28c6.json</td><td>Tenant__1_-f28c6.json</td><td>215879</td></tr><tr><td>dbfs:/FileStore/tables/Test.csv</td><td>Test.csv</td><td>7745711</td></tr><tr><td>dbfs:/FileStore/tables/Training.csv</td><td>Training.csv</td><td>350751</td></tr><tr><td>dbfs:/FileStore/tables/Users_Report_2018_03_25-2a093.csv</td><td>Users_Report_2018_03_25-2a093.csv</td><td>55</td></tr><tr><td>dbfs:/FileStore/tables/WA_Fn_UseC__Telco_Customer_Churn-89c80.csv</td><td>WA_Fn_UseC__Telco_Customer_Churn-89c80.csv</td><td>977501</td></tr><tr><td>dbfs:/FileStore/tables/births_train_csv-dc6be.gz</td><td>births_train_csv-dc6be.gz</td><td>931988</td></tr><tr><td>dbfs:/FileStore/tables/creditcard.csv</td><td>creditcard.csv</td><td>150828752</td></tr><tr><td>dbfs:/FileStore/tables/hack_data.csv</td><td>hack_data.csv</td><td>14268</td></tr><tr><td>dbfs:/FileStore/tables/ham</td><td>ham</td><td>355018</td></tr><tr><td>dbfs:/FileStore/tables/kc_house_data.csv</td><td>kc_house_data.csv</td><td>2515206</td></tr><tr><td>dbfs:/FileStore/tables/spam</td><td>spam</td><td>105420</td></tr><tr><td>dbfs:/FileStore/tables/userSCV_matching.csv</td><td>userSCV_matching.csv</td><td>2328</td></tr><tr><td>dbfs:/FileStore/tables/user_criteria1.csv</td><td>user_criteria1.csv</td><td>609</td></tr><tr><td>dbfs:/FileStore/tables/user_criteria2.csv</td><td>user_criteria2.csv</td><td>394</td></tr><tr><td>dbfs:/FileStore/tables/user_criteria3.csv</td><td>user_criteria3.csv</td><td>400</td></tr><tr><td>dbfs:/FileStore/tables/user_criteria4.csv</td><td>user_criteria4.csv</td><td>408</td></tr><tr><td>dbfs:/FileStore/tables/users_load.csv</td><td>users_load.csv</td><td>109006</td></tr><tr><td>dbfs:/FileStore/tables/users_load1_1.csv</td><td>users_load1_1.csv</td><td>2353</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nsc = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a56c5a7e-2e97-45ac-b378-30038bd60235"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import HiveContext\nhivecontext=HiveContext(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e40c2ce-c076-437f-9b42-3f14a66c0e8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.setConf('hive.support.concurrency','true');\nhivecontext.setConf('hive.enforce.bucketing','true');\nhivecontext.setConf('hive.exec.dynamic.partition.mode','nostrict');\nhivecontext.setConf('hive.compactor.initiator.on','true');\nhivecontext.setConf('hive.compactor.worker.threads','1');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86cb18d2-a14b-42fb-9ce8-0c4f37fec4a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql('use default')\nhivecontext.sql('show tables').show()\n# vecontext.sql('drop table sample_database.new_sample')\n# econtext.sql('drop database sample_database')\n# hivecontext.sql('create database sample_database')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":null,"showTitle":false,"inputWidgets":{},"nuid":"1adbff1c-f6ad-424c-b431-fb7afff9dedb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|      apartment_data|      false|\n| default|     apartment_delta|      false|\n| default|         apartment_i|      false|\n| default|apartment_mainten...|      false|\n| default|apartment_mainten...|      false|\n| default|apartment_mainten...|      false|\n| default| apt_maintenace_data|      false|\n| default|       building_data|      false|\n| default|          building_i|      false|\n| default|c1_firstname_ip_o...|      false|\n| default|c1_firstname_last...|      false|\n| default|c1_firstname_post...|      false|\n| default|c1_firstname_user...|      false|\n| default|c1_firstname_user...|      false|\n| default|   c1_fn_mobile_data|      false|\n| default|c1_postcode_addr1...|      false|\n| default|c1_postcode_dob_data|      false|\n| default|     contractor_data|      false|\n| default|    contractor_delta|      false|\n| default|        contractor_i|      false|\n+--------+--------------------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+--------------------+-----------+\ndatabase|           tableName|isTemporary|\n+--------+--------------------+-----------+\n default|      apartment_data|      false|\n default|     apartment_delta|      false|\n default|         apartment_i|      false|\n default|apartment_mainten...|      false|\n default|apartment_mainten...|      false|\n default|apartment_mainten...|      false|\n default| apt_maintenace_data|      false|\n default|       building_data|      false|\n default|          building_i|      false|\n default|c1_firstname_ip_o...|      false|\n default|c1_firstname_last...|      false|\n default|c1_firstname_post...|      false|\n default|c1_firstname_user...|      false|\n default|c1_firstname_user...|      false|\n default|   c1_fn_mobile_data|      false|\n default|c1_postcode_addr1...|      false|\n default|c1_postcode_dob_data|      false|\n default|     contractor_data|      false|\n default|    contractor_delta|      false|\n default|        contractor_i|      false|\n+--------+--------------------+-----------+\nonly showing top 20 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql('CREATE TABLE new_sample ( \\\n   city\tSTRING, \\\n   population INT \\\n) \\\nPARTITIONED BY (country STRING) tblproperties(\"skip.header.line.count\"=\"1\") ')\n\nhivecontext.sql('show tables').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aff96d31-309e-41ff-b52d-832a601e2c0c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o178.sql.\n: org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view &#39;new_sample&#39; already exists in database &#39;default&#39;;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:308)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:104)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:330)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:316)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:23)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:98)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:327)\n\tat com.databricks.sql.DatabricksSessionCatalog.createTable(DatabricksSessionCatalog.scala:117)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:130)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:81)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3334)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3333)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:653)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1202827879069657&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>    population INT<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> ) \\\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> PARTITIONED BY (country STRING) tblproperties(&#34;skip.header.line.count&#34;=&#34;1&#34;) &#39;)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> hivecontext<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;show tables&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    351</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 353</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    354</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    746</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    747</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 748</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    749</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     69</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 71</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     73</span>                 <span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;Table or view &#39;new_sample&#39; already exists in database &#39;default&#39;;&#34;</div>","errorSummary":"org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view &#39;new_sample&#39; already exists in database &#39;default&#39;;","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o178.sql.\n: org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view &#39;new_sample&#39; already exists in database &#39;default&#39;;\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:308)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:104)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:330)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:316)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:23)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:298)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:98)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:327)\n\tat com.databricks.sql.DatabricksSessionCatalog.createTable(DatabricksSessionCatalog.scala:117)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:130)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:81)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3334)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3333)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:195)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:653)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1202827879069657&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>    population INT<span class=\"ansi-red-fg\"> </span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> ) \\\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> PARTITIONED BY (country STRING) tblproperties(&#34;skip.header.line.count&#34;=&#34;1&#34;) &#39;)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> hivecontext<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;show tables&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    351</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    352</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 353</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    354</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    355</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">sql</span><span class=\"ansi-blue-fg\">(self, sqlQuery)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    746</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row1&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row2&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>f1<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span> f2<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;row3&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    747</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 748</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span>sqlQuery<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_wrapped<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    749</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2.0</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     69</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 71</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     73</span>                 <span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;Table or view &#39;new_sample&#39; already exists in database &#39;default&#39;;&#34;</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df=hivecontext.sql('INSERT INTO TABLE new_sample PARTITION (country)    SELECT city,population,country FROM  sample_csv');\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ecc5ea5-25f1-443b-9847-478617c71f75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql(\"select count(*) from sample_database.new_sample\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":null,"showTitle":false,"inputWidgets":{},"nuid":"5f657029-d33a-4f30-82ac-c1d2d21aa41d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql(\"select * from sample_database.new_sample limit 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":null,"showTitle":false,"inputWidgets":{},"nuid":"0e1f885f-0f43-43d0-81c7-2d88cb784cfc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n\nschema = StructType([\n            StructField(\"city\", StringType(), True),\n            StructField(\"country\", StringType(), True),\n            StructField(\"population\", IntegerType(), True)])\n\ncountries = ['India', 'USA', 'Brazil', 'Spain']\ncities = ['Bangalore', 'New York', '   Sao Paulo   ', 'Madrid']\npopulation = [422300000,134795791,12341418,6489162]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aec514ff-db89-4032-a0db-1842e737ffbb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = sc.createDataFrame(list(zip(cities, countries, population)), schema=schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a179d53f-0f98-404d-9046-c5e3a1e2bfb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df.registerTempTable('update_dataframe')\ndf.printSchema()\n\ndf_filter = df.filter(df.population.isin(6489162))\nprint(df_filter)\ndf_filter.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c7ae8a3-919d-45c6-b3cc-60160b3e2a21"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql('INSERT OVERWRITE TABLE new_sample PARTITION (country) \\\n                   SELECT city,population,country \\\n                   FROM update_dataframe')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c128ea54-8bd3-4bbc-bdcf-3c78ba360d25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["hivecontext.sql(\"select * from new_sample limit 10\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9387d738-67d2-4b4a-9be6-218e8eda4446"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Hive example to Update Table partition","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744917888}},"nbformat":4,"nbformat_minor":0}
