{"cells":[{"cell_type":"code","source":["from pyspark import SparkConf,SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql import Row,SQLContext\nimport sys\nimport requests\n# create spark configuration\nconf = SparkConf()\nconf.setAppName(\"TwitterStreamApp\")\n# create spark context with the above configuration\n# create the Streaming Context from the above spark context with interval size 2 seconds\nssc = StreamingContext(sc, 2)\n# setting a checkpoint to allow RDD recovery\nssc.checkpoint(\"/csv\")\n# read data from port 9009\ndataStream = ssc.socketTextStream(\"localhost\",9009)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60996660-2c4d-48fe-8464-2d4e0624bbdf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def aggregate_tags_count(new_values, total_sum):\n\treturn sum(new_values) + (total_sum or 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86804fa2-fedd-498b-802d-c39566f96484"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def send_df_to_dashboard(df):\n  # extract the hashtags from dataframe and convert them into array\n  top_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n  # extract the counts from dataframe and convert them into array\n  tags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n  print(top_tags)\n   print(tags_count)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a236cb21-da1a-43ba-b8d6-fd190abc43a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">If you want to paste code into IPython, try the %paste and %cpaste magic functions.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">If you want to paste code into IPython, try the %paste and %cpaste magic functions.\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;command-881310835193571&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">7</span>\n<span class=\"ansiyellow\">    print(tags_count)</span>\n<span class=\"ansigrey\">                     ^</span>\n<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unindent does not match any outer indentation level\n</div>","errorSummary":"<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unindent does not match any outer indentation level","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;command-881310835193571&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">7</span>\n<span class=\"ansiyellow\">    print(tags_count)</span>\n<span class=\"ansigrey\">                     ^</span>\n<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unindent does not match any outer indentation level\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def get_sql_context_instance(spark_context):\n  if ('sqlContextSingletonInstance' not in globals()):\n    globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n    return globals()['sqlContextSingletonInstance']\n\ndef process_rdd(time, rdd):\n  print(\"----------- %s -----------\" % str(time))\n  try:\n    # Get spark sql singleton context from the current context\n    sql_context = get_sql_context_instance(rdd.context)\n    # convert the RDD to Row RDD\n    row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n    # create a DF from the Row RDD\n    hashtags_df = sql_context.createDataFrame(row_rdd)\n    # Register the dataframe as table\n    hashtags_df.registerTempTable(\"hashtags\")\n    # get the top 10 hashtags from the table using SQL and print them\n    hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n    hashtag_counts_df.show()\n    # call this method to prepare top 10 hashtags DF and send them\n    send_df_to_dashboard(hashtag_counts_df)\n  except:\n    e = sys.exc_info()[0]\n    print(\"Error: %s\" % e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f42b021-b817-43fe-a951-f2fe935c0ec9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# split each tweet into words\nwords = dataStream.flatMap(lambda line: line.split(\" \"))\n# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\nhashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n# adding the count of each hashtag to its last count\ntags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n# do processing for each RDD generated in each interval\ntags_totals.foreachRDD(process_rdd)\n# start the streaming computation\nssc.start()\n# wait for the streaming to finish\nssc.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1336ade0-bf60-4616-89c2-2027353b693e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-881310835193567&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> hashtags <span class=\"ansiyellow\">=</span> words<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> w<span class=\"ansiyellow\">:</span> <span class=\"ansiblue\">&apos;#&apos;</span> <span class=\"ansigreen\">in</span> w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansired\"># adding the count of each hashtag to its last count</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>tags_totals <span class=\"ansiyellow\">=</span> hashtags<span class=\"ansiyellow\">.</span>updateStateByKey<span class=\"ansiyellow\">(</span>aggregate_tags_count<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansired\"># do processing for each RDD generated in each interval</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> tags_totals<span class=\"ansiyellow\">.</span>foreachRDD<span class=\"ansiyellow\">(</span>process_rdd<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/dstream.pyc</span> in <span class=\"ansicyan\">updateStateByKey</span><span class=\"ansiblue\">(self, updateFunc, numPartitions, initialRDD)</span>\n<span class=\"ansigreen\">    602</span>                                                        initialRDD._jrdd)\n<span class=\"ansigreen\">    603</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 604</span><span class=\"ansiyellow\">             </span>dstream <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonStateDStream<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jdstream<span class=\"ansiyellow\">.</span>dstream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> jreduceFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    606</span>         <span class=\"ansigreen\">return</span> DStream<span class=\"ansiyellow\">(</span>dstream<span class=\"ansiyellow\">.</span>asJavaDStream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_ssc<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>serializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/dstream.pyc</span> in <span class=\"ansicyan\">_jdstream</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    641</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    642</span>         jfunc <span class=\"ansiyellow\">=</span> TransformFunction<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>func<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>prev<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 643</span><span class=\"ansiyellow\">         </span>dstream <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonTransformedDStream<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>prev<span class=\"ansiyellow\">.</span>_jdstream<span class=\"ansiyellow\">.</span>dstream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> jfunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>         self<span class=\"ansiyellow\">.</span>_jdstream_val <span class=\"ansiyellow\">=</span> dstream<span class=\"ansiyellow\">.</span>asJavaDStream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    645</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_jdstream_val<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1426</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1427</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1428</span><span class=\"ansiyellow\">             answer, self._gateway_client, None, self._fqn)\n</span><span class=\"ansigreen\">   1429</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1430</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:224)\n\tat org.apache.spark.streaming.dstream.DStream.&lt;init&gt;(DStream.scala:66)\n\tat org.apache.spark.streaming.api.python.PythonDStream.&lt;init&gt;(PythonDStream.scala:224)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.&lt;init&gt;(PythonDStream.scala:241)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:250)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-881310835193567&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> hashtags <span class=\"ansiyellow\">=</span> words<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> w<span class=\"ansiyellow\">:</span> <span class=\"ansiblue\">&apos;#&apos;</span> <span class=\"ansigreen\">in</span> w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansired\"># adding the count of each hashtag to its last count</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>tags_totals <span class=\"ansiyellow\">=</span> hashtags<span class=\"ansiyellow\">.</span>updateStateByKey<span class=\"ansiyellow\">(</span>aggregate_tags_count<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansired\"># do processing for each RDD generated in each interval</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> tags_totals<span class=\"ansiyellow\">.</span>foreachRDD<span class=\"ansiyellow\">(</span>process_rdd<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/dstream.pyc</span> in <span class=\"ansicyan\">updateStateByKey</span><span class=\"ansiblue\">(self, updateFunc, numPartitions, initialRDD)</span>\n<span class=\"ansigreen\">    602</span>                                                        initialRDD._jrdd)\n<span class=\"ansigreen\">    603</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 604</span><span class=\"ansiyellow\">             </span>dstream <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonStateDStream<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jdstream<span class=\"ansiyellow\">.</span>dstream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> jreduceFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    606</span>         <span class=\"ansigreen\">return</span> DStream<span class=\"ansiyellow\">(</span>dstream<span class=\"ansiyellow\">.</span>asJavaDStream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_ssc<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>serializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/dstream.pyc</span> in <span class=\"ansicyan\">_jdstream</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    641</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    642</span>         jfunc <span class=\"ansiyellow\">=</span> TransformFunction<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>func<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>prev<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 643</span><span class=\"ansiyellow\">         </span>dstream <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonTransformedDStream<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>prev<span class=\"ansiyellow\">.</span>_jdstream<span class=\"ansiyellow\">.</span>dstream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> jfunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>         self<span class=\"ansiyellow\">.</span>_jdstream_val <span class=\"ansiyellow\">=</span> dstream<span class=\"ansiyellow\">.</span>asJavaDStream<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    645</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_jdstream_val<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1426</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1427</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1428</span><span class=\"ansiyellow\">             answer, self._gateway_client, None, self._fqn)\n</span><span class=\"ansigreen\">   1429</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1430</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling None.org.apache.spark.streaming.api.python.PythonTransformedDStream.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after starting a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:224)\n\tat org.apache.spark.streaming.dstream.DStream.&lt;init&gt;(DStream.scala:66)\n\tat org.apache.spark.streaming.api.python.PythonDStream.&lt;init&gt;(PythonDStream.scala:224)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.&lt;init&gt;(PythonDStream.scala:241)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:250)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91d5b9e5-06ab-4490-815c-fc7d78261b4a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TwitterStreaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744917847}},"nbformat":4,"nbformat_minor":0}
