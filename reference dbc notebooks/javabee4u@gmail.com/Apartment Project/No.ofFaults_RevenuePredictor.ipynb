{"cells":[{"cell_type":"markdown","source":["*Problem Statement*: Predict No.of faults, revenue and store it in hive table by reading incoming data from Producer as Stream. Predict No.of faults, revenue is done by using Spark ML Linear regression. \n1. Launch API Producer -> 2. Read data from API as stream -> 3. Build model -> 4. apply Linear regression Algorithm -> 5. Save the results in hive Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"107ba36a-b733-4f10-a636-380644b77bd7"}}},{"cell_type":"code","source":["import requests\nimport json\nfrom pyspark.sql.types import StringType, IntegerType, TimestampType, DateType, DoubleType, StructType, StructField\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window as W\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import rank, col\nimport datetime\nimport time\nimport re\nfrom pyspark.streaming import StreamingContext\nimport pprint\nfrom pyspark.mllib.stat import Statistics\nimport numpy as np\n  \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31a25d64-9e32-4161-8e5b-7a91c61c6c77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def convertCharges(charges):\n  if (len(charges) > 0):\n    try:\n      obj = charges.partition(\"£\")[2]\n      return obj\n    except UnicodeEncodeError:\n        return obj.encode('ascii', 'ignore').decode('ascii')   \n  else: \n    return \"0\"\n  \nudfConvertCharges = udf(convertCharges, StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08f0dd19-151f-4968-8740-56dc1db02122"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Build datasets \ndef create_category_vars( dataset, field_name ):\n  idx_col = field_name + \"Index\"\n  col_vec = field_name + \"Vec\"\n\n  month_stringIndexer = StringIndexer( inputCol=field_name,\n                                       outputCol=idx_col )\n\n  month_model = month_stringIndexer.fit( dataset )\n  month_indexed = month_model.transform( dataset )\n\n  month_encoder = OneHotEncoder( dropLast=True,\n                                 inputCol=idx_col,\n                                 outputCol= col_vec )\n\n  return month_encoder.transform( month_indexed )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5c4a3c3-4c94-481d-9009-53171c45cd72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def build_model(tableName):\n  maintenance_sql = \"select * from \" + tableName\n  df_maintenance = spark.sql(maintenance_sql)\n  df_maintenance = df_maintenance.withColumn(\"Charges_incurred\", udfConvertCharges(\"Charges_incurred\") )\n  df = df_maintenance.select(col('Maintenance_id'), col('Apartment_number'), col('Mdate'), col('Issue_reported'), \\\n                           col('Contractor_id'), col('Resolution'), col('Status'), \\\n                           df_maintenance.Charges_incurred.cast('float').alias('Charges_incurred'))\n\n  df.show()\n  # Find the correlation between house price and sqft_living\n  column_labels = ['Apartment_number','Mdate', 'Issue_reported', 'Contractor_id','Resolution', \\\n         'Status', 'Charges_incurred']\n  df.stat.corr( 'Apartment_number', 'Charges_incurred' )\n  # find which features highly correlated with price One can use many methoods to fidn co relations between the columns \n  column_corr = Statistics.corr(df.rdd.map(lambda x:\n                         np.array([x['Apartment_number'],\n                                   x['Contractor_id'],\n                                   x['Charges_incurred']\n                                  ])), method='pearson')\n  # categorize continuous features and categorical features\n  continuous_features = ['Apartment_number',  'Contractor_id', 'Charges_incurred']\n\n  categorical_features = ['Status']\n  # OneHot Encoding for all the categorical columns\n  # Encoding - convert any text value within columns into the numbers Spark supply function - OneHotEncoder\n  from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, PolynomialExpansion, VectorIndexer\n\n  for columnName in categorical_features:\n    df = create_category_vars( df, columnName )\n\n  df.cache()\n  # create features for vector Assembler\n  featureCols = continuous_features + ['StatusVec']\n  assembler = VectorAssembler( inputCols = featureCols, outputCol = \"features\")\n  maintenance_train_df = assembler.transform( df )\n  # Create label\n  from pyspark.sql.functions import round\n\n  maintenance_train_df = maintenance_train_df.withColumn( \"label\", round('Charges_incurred', 4) )\n  # split the dataset\n  train_df, test_df = maintenance_train_df.randomSplit( [0.7, 0.3], seed = 30 )\n  # build the linear regression model\n  from pyspark.ml.regression import LinearRegression\n  # regParam=0.0 \n  linreg = LinearRegression(maxIter=500, regParam=0.0)\n  lm = linreg.fit( train_df )\n  trainingSummary = lm.summary\n  print(\"numIterations: %d\" % trainingSummary.totalIterations)\n  print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n  trainingSummary.residuals.show()\n  print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n  print(\"r2: %f\" % trainingSummary.r2)\n  \n  lm.coefficients\n  # make predictions for test data and evaluate\n  y_pred = lm.transform( test_df )\n  from pyspark.sql.functions import month, sum\n\n  (y_pred\n    .groupBy(month(\"Mdate\").alias(\"Month\"))\n    .agg(sum(\"prediction\").alias(\"Expected Revenue\"))\n    .show())\n  \n  # store the predicted value in table.\n  if (len(spark.sql(\"SHOW TABLES LIKE '\" + \"apt_maintenace_prediction\"+ \"'\").collect()) == 1):\n      y_pred.write.insertInto(\"apt_maintenace_prediction\")\n  else:\n      y_pred.write.saveAsTable(\"apt_maintenace_prediction\")\n  \n  # calculate actual predicted price\n  from pyspark.sql.functions import exp\n\n  y_pred = y_pred.withColumn( \"y_pred\", exp( 'prediction' ) )\n\n  # calculate RMSE\n  from pyspark.ml.evaluation import RegressionEvaluator\n  rmse_evaluator = RegressionEvaluator(labelCol=\"Charges_incurred\",\n                              predictionCol=\"y_pred\",\n                              metricName=\"rmse\" )\n  lm_rmse = rmse_evaluator.evaluate( y_pred )\n  lm_rmse\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76fba0b9-9d1c-47e1-9598-596453ca395a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def processD(apt_maintenance):\n  apt_maintenance_map = apt_maintenance.filter(lambda l: len(l) == 8).map(lambda p: Row(\\\n                                         Maintenance_id=p[0], Apartment_number=p[1], Mdate=p[2], \\\n                                         Issue_reported=p[3],Contractor_id=p[4], Resolution=p[5], \\\n                                         Status=p[6], Charges_incurred=p[7]))\n\n  #apt_maintenance_map = apt_maintenance.map(lambda p: Row(Maintenance_id=p[0], Apartment_number=p[1], Mdate=p[2], \\\n  #                                       Issue_reported=p[3],Contractor_id=p[4], Resolution=p[5], \\\n  #                                       Status=p[6], Charges_incurred=p[7]))\n  #apt_maintenance_map_filter = apt_maintenance_map.filter(lambda line: len(line) == 8)\n\n  count = apt_maintenance_map.count()\n  if (count > 0):\n    print(\"count of records:\" + str(count))\n    apt_maint_df = sqlContext.createDataFrame(apt_maintenance_map)\n     \n    if (len(spark.sql(\"SHOW TABLES LIKE '\" + \"apt_maintenace_data\"+ \"'\").collect()) == 1):\n      apt_maint_df.write.insertInto(\"apt_maintenace_data\")\n    else:\n      apt_maint_df.write.saveAsTable(\"apt_maintenace_data\")\n    build_model(\"apt_maintenace_data\")\n  else:\n    print(\"RDD is empty\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a2cf58c-c2dc-4e60-b8de-65e95aa44267"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Schema for Apartment Maintenance\napt_maintenance_schema = StructType([\n            StructField(\"Maintenance_id\", IntegerType(), True),\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Mdate\", StringType(), True),\n            StructField(\"Issue_reported\", StringType(), True),\n            StructField(\"Contractor_id\", IntegerType(), True), \n            StructField(\"Resolution\", StringType(), True), \n            StructField(\"Status\", StringType(), True),\n            StructField(\"Charges_incurred\", StringType(), True),\n            StructField(\"event_time\", StringType(), True)])\n\napt_maintenance_path = \"/FileStore/users/apt_maintenance/inprogress\" \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"657b5281-bce8-4bc4-8d3e-07c510d44cc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# streaming starts here by reading the input files \napt_maint_df = (\n  spark\n    .readStream\n    .schema(apt_maintenance_schema)\n    .option(\"maxFilesPerTrigger\", \"1\")\n    .option(\"header\", \"true\")\n    .csv(apt_maintenance_path)\n)\napt_maint_df.registerTempTable(\"apt_maintenance\")\nquery = apt_maint_df.writeStream.outputMode(\"append\").format(\"console\").start()\nquery.awaitTermination()\napt_maint_df.show()\n\nbuild_model(\"apt_maintenance\")\nif (len(spark.sql(\"SHOW TABLES LIKE '\" + \"apt_maintenace_data\"+ \"'\").collect()) == 1):\n  apt_maint_df.write.insertInto(\"apt_maintenace_data\")\nelse:\n  apt_maint_df.write.saveAsTable(\"apt_maintenace_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38de68b3-a266-4dae-a4f7-c5a2a9e2d362"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# streaming starts here by reading the input files \napt_stream_df =  ( spark.readStream \\\n  .schema(apt_maintenance_schema) \\\n  .option(\"maxFilesPerTrigger\", \"1\") \\\n  .option(\"header\", \"true\") \\\n  .option(\"multiLine\", \"true\") \\\n  .csv(apt_maintenance_path) )\n  \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d353491d-bb91-46ac-b24d-743c98feddcb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import schedule\nimport time\ndef job():\n  query = apt_stream_df \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\n   \n  apt_stream_df.show()\n  print(apt_stream_df.count())\nschedule.every(10).seconds.do(job)\n\nwhile True:\n  schedule.run_pending()\n  time.sleep(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"becd5685-2df3-45ee-bd50-3119b93943c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from io import StringIO\nimport csv\nbatchIntervalSeconds = 10 \nfileprefix = apt_maintenance_path\n\nssc = StreamingContext(sc, batchIntervalSeconds)\n\n# Set each DStreams in this context to remember RDDs it generated in the last given duration.\n# DStreams remember RDDs only for a limited duration of time and releases them for garbage\n# collection. This method allows the developer to specify how long to remember the RDDs (\n# if the developer wishes to query old data outside the DStream computation).\n\n\nuserDStream = ssc.textFileStream(fileprefix)\n# lines = [v for v in csv.reader(StringIO(userDStream.encode('utf8', 'ignore')))]\n\nsplit_users = userDStream.map(lambda l: l.split(\",\"))\n\n#files = userDStream.foreachRDD(fileName)\nsplit_users.foreachRDD(processD)\n#print(rows)\n   \n#rows.foreachRDD(process)\nssc.start()\nssc.awaitTerminationOrTimeout(200)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d26235c-5b90-4a72-b1c4-f7a0bfc20462"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">RDD is empty\ncount of records:19\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\n|      Maintenance_id|   Apartment_number|               Mdate|      Issue_reported|       Contractor_id|          Resolution|              Status|Charges_incurred|\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\n|                   2|                302| 2017-08-24 06:49:41|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n|Fusce posuere fel...|         laoreet ut|     rhoncus aliquet|        pulvinar sed| nisl. Nunc rhonc...|                Open|             £733.48|            null|\n|                  10|                386| 2017-09-28 13:15:41|\u0000Praesent blandit...|        lacinia eget|      tincidunt eget|          tempus vel|            null|\n|Quisque id justo ...|    sollicitudin ut|          suscipit a|          feugiat et|              eros.\u0000|              Closed|             £536.78|            null|\n|Aenean fermentum....|     convallis eget|     eleifend luctus|        ultricies eu|              nibh.\u0000|            Assigned|             £507.51|            null|\n|                  14|                593| 2017-04-22 13:03:55|\u0000Fusce posuere fe...|          laoreet ut|     rhoncus aliquet|        pulvinar sed|            null|\n|                  15|                 27| 2017-10-08 21:15:06|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n|                  20|                143| 2017-04-21 07:15:32|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n|                  22|                110| 2017-07-23 06:56:14|\u0000Morbi non lectus...|       venenatis non|         sodales sed|        tincidunt eu|            null|\n|Praesent blandit....|       lacinia eget|      tincidunt eget|          tempus vel|              pede.\u0000|              Closed|             £901.54|            null|\n|Praesent blandit....|       lacinia eget|      tincidunt eget|          tempus vel|              pede.\u0000|              Closed|              £46.81|            null|\n|     Mauris enim leo|        rhoncus sed| vestibulum sit amet|           cursus id| turpis. Integer ...| massa id loborti...| tortor risus dap...|            null|\n|      Proin leo odio|       porttitor id|        consequat in|        consequat ut| nulla. Sed accum...|              Closed|             £631.38|            null|\n|Curabitur gravida...| sollicitudin vitae|   consectetuer eget|           rutrum at|             lorem.\u0000|              Closed|             £348.14|            null|\n|    Donec diam neque|    vestibulum eget|        vulputate ut|        ultrices vel| augue. Vestibulu...| magna vestibulum...| erat tortor soll...|            null|\n|                  44|                243| 2017-01-02 22:55:13|\u0000Vestibulum quam ...|           varius ut|         blandit non|         interdum in|            null|\n|     Mauris enim leo|        rhoncus sed| vestibulum sit amet|           cursus id| turpis. Integer ...| massa id loborti...| tortor risus dap...|            null|\n|                  48|                515| 2017-08-15 16:10:08|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n|    Donec diam neque|    vestibulum eget|        vulputate ut|        ultrices vel| augue. Vestibulu...| magna vestibulum...| erat tortor soll...|            null|\n|In sagittis dui v...|         aliquet at|         feugiat non|        pretium quis|            lectus.\u0000|              Closed|             £589.42|            null|\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\nonly showing top 20 rows\n\nRDD is empty\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">RDD is empty\ncount of records:19\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\n      Maintenance_id|   Apartment_number|               Mdate|      Issue_reported|       Contractor_id|          Resolution|              Status|Charges_incurred|\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\n                   2|                302| 2017-08-24 06:49:41|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\nFusce posuere fel...|         laoreet ut|     rhoncus aliquet|        pulvinar sed| nisl. Nunc rhonc...|                Open|             £733.48|            null|\n                  10|                386| 2017-09-28 13:15:41|\u0000Praesent blandit...|        lacinia eget|      tincidunt eget|          tempus vel|            null|\nQuisque id justo ...|    sollicitudin ut|          suscipit a|          feugiat et|              eros.\u0000|              Closed|             £536.78|            null|\nAenean fermentum....|     convallis eget|     eleifend luctus|        ultricies eu|              nibh.\u0000|            Assigned|             £507.51|            null|\n                  14|                593| 2017-04-22 13:03:55|\u0000Fusce posuere fe...|          laoreet ut|     rhoncus aliquet|        pulvinar sed|            null|\n                  15|                 27| 2017-10-08 21:15:06|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n                  20|                143| 2017-04-21 07:15:32|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n                  22|                110| 2017-07-23 06:56:14|\u0000Morbi non lectus...|       venenatis non|         sodales sed|        tincidunt eu|            null|\nPraesent blandit....|       lacinia eget|      tincidunt eget|          tempus vel|              pede.\u0000|              Closed|             £901.54|            null|\nPraesent blandit....|       lacinia eget|      tincidunt eget|          tempus vel|              pede.\u0000|              Closed|              £46.81|            null|\n     Mauris enim leo|        rhoncus sed| vestibulum sit amet|           cursus id| turpis. Integer ...| massa id loborti...| tortor risus dap...|            null|\n      Proin leo odio|       porttitor id|        consequat in|        consequat ut| nulla. Sed accum...|              Closed|             £631.38|            null|\nCurabitur gravida...| sollicitudin vitae|   consectetuer eget|           rutrum at|             lorem.\u0000|              Closed|             £348.14|            null|\n    Donec diam neque|    vestibulum eget|        vulputate ut|        ultrices vel| augue. Vestibulu...| magna vestibulum...| erat tortor soll...|            null|\n                  44|                243| 2017-01-02 22:55:13|\u0000Vestibulum quam ...|           varius ut|         blandit non|         interdum in|            null|\n     Mauris enim leo|        rhoncus sed| vestibulum sit amet|           cursus id| turpis. Integer ...| massa id loborti...| tortor risus dap...|            null|\n                  48|                515| 2017-08-15 16:10:08|\u0000Morbi porttitor ...|          auctor sed|        tristique in|     tempus sit amet|            null|\n    Donec diam neque|    vestibulum eget|        vulputate ut|        ultrices vel| augue. Vestibulu...| magna vestibulum...| erat tortor soll...|            null|\nIn sagittis dui v...|         aliquet at|         feugiat non|        pretium quis|            lectus.\u0000|              Closed|             £589.42|            null|\n+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+\nonly showing top 20 rows\n\nRDD is empty\n</div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3248458478020622&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     23</span> <span class=\"ansired\">#rows.foreachRDD(process)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     24</span> ssc<span class=\"ansiyellow\">.</span>start<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 25</span><span class=\"ansiyellow\"> </span>ssc<span class=\"ansiyellow\">.</span>awaitTerminationOrTimeout<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">200</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     26</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/context.py</span> in <span class=\"ansicyan\">awaitTerminationOrTimeout</span><span class=\"ansiblue\">(self, timeout)</span>\n<span class=\"ansigreen\">    216</span>         <span class=\"ansiyellow\">@</span>param timeout<span class=\"ansiyellow\">:</span> time to wait <span class=\"ansigreen\">in</span> seconds<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    217</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 218</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_jssc<span class=\"ansiyellow\">.</span>awaitTerminationOrTimeout<span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>timeout <span class=\"ansiyellow\">*</span> <span class=\"ansicyan\">1000</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    219</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    220</span>     <span class=\"ansigreen\">def</span> stop<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> stopSparkContext<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">True</span><span class=\"ansiyellow\">,</span> stopGraceFully<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o228.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/sql/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py&quot;, line 320, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o417.corr.\n: java.lang.IllegalArgumentException: requirement failed: Currently correlation calculation for columns with dataType StringType not supported.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:159)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:157)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.collectStatisticalData(StatFunctions.scala:157)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.pearsonCorrelation(StatFunctions.scala:109)\n\tat org.apache.spark.sql.DataFrameStatFunctions.corr(DataFrameStatFunctions.scala:160)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/streaming/util.py&quot;, line 65, in call\n    r = self.func(t, *rdds)\n  File &quot;/databricks/spark/python/pyspark/streaming/dstream.py&quot;, line 159, in &lt;lambda&gt;\n    func = lambda t, rdd: old_func(rdd)\n  File &quot;&lt;command-3248458478020623&gt;&quot;, line 18, in processD\n    build_model(&quot;apt_maintenace_data&quot;)\n  File &quot;&lt;command-3248458478020624&gt;&quot;, line 10, in build_model\n    df.stat.corr( &apos;Apartment_number&apos;, &apos;Charges_incurred&apos; )\n  File &quot;/databricks/spark/python/pyspark/sql/dataframe.py&quot;, line 2155, in corr\n    return self.df.corr(col1, col2, method)\n  File &quot;/databricks/spark/python/pyspark/sql/dataframe.py&quot;, line 1831, in corr\n    return self._jdf.stat().corr(col1, col2, method)\n  File &quot;/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py&quot;, line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;/databricks/spark/python/pyspark/sql/utils.py&quot;, line 79, in deco\n    raise IllegalArgumentException(s.split(&apos;: &apos;, 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: &apos;requirement failed: Currently correlation calculation for columns with dataType StringType not supported.&apos;\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"org.apache.spark.SparkException: An exception was raised by Python:","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3248458478020622&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     23</span> <span class=\"ansired\">#rows.foreachRDD(process)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     24</span> ssc<span class=\"ansiyellow\">.</span>start<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 25</span><span class=\"ansiyellow\"> </span>ssc<span class=\"ansiyellow\">.</span>awaitTerminationOrTimeout<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">200</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     26</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/streaming/context.py</span> in <span class=\"ansicyan\">awaitTerminationOrTimeout</span><span class=\"ansiblue\">(self, timeout)</span>\n<span class=\"ansigreen\">    216</span>         <span class=\"ansiyellow\">@</span>param timeout<span class=\"ansiyellow\">:</span> time to wait <span class=\"ansigreen\">in</span> seconds<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    217</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 218</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_jssc<span class=\"ansiyellow\">.</span>awaitTerminationOrTimeout<span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>timeout <span class=\"ansiyellow\">*</span> <span class=\"ansicyan\">1000</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    219</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    220</span>     <span class=\"ansigreen\">def</span> stop<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> stopSparkContext<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">True</span><span class=\"ansiyellow\">,</span> stopGraceFully<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o228.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/sql/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py&quot;, line 320, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o417.corr.\n: java.lang.IllegalArgumentException: requirement failed: Currently correlation calculation for columns with dataType StringType not supported.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:159)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:157)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.collectStatisticalData(StatFunctions.scala:157)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.pearsonCorrelation(StatFunctions.scala:109)\n\tat org.apache.spark.sql.DataFrameStatFunctions.corr(DataFrameStatFunctions.scala:160)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/streaming/util.py&quot;, line 65, in call\n    r = self.func(t, *rdds)\n  File &quot;/databricks/spark/python/pyspark/streaming/dstream.py&quot;, line 159, in &lt;lambda&gt;\n    func = lambda t, rdd: old_func(rdd)\n  File &quot;&lt;command-3248458478020623&gt;&quot;, line 18, in processD\n    build_model(&quot;apt_maintenace_data&quot;)\n  File &quot;&lt;command-3248458478020624&gt;&quot;, line 10, in build_model\n    df.stat.corr( &apos;Apartment_number&apos;, &apos;Charges_incurred&apos; )\n  File &quot;/databricks/spark/python/pyspark/sql/dataframe.py&quot;, line 2155, in corr\n    return self.df.corr(col1, col2, method)\n  File &quot;/databricks/spark/python/pyspark/sql/dataframe.py&quot;, line 1831, in corr\n    return self._jdf.stat().corr(col1, col2, method)\n  File &quot;/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py&quot;, line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;/databricks/spark/python/pyspark/sql/utils.py&quot;, line 79, in deco\n    raise IllegalArgumentException(s.split(&apos;: &apos;, 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: &apos;requirement failed: Currently correlation calculation for columns with dataType StringType not supported.&apos;\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# calcualte R-Squared - Diff between predicted value and actual value\nr2_evaluator = RegressionEvaluator(labelCol=\"Charges_incurred\",\n                              predictionCol=\"y_pred\",\n                              metricName=\"r2\" )\nlm_r2 = r2_evaluator.evaluate( y_pred )\nlm_r2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c302d5ad-899d-414f-865d-f03b8b67aad8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_r2_rmse( model, test_df ):\n  y_pred = model.transform( test_df )\n  y_pred = y_pred.withColumn( \"y_pred\", exp( 'prediction' ) )\n  rmse_evaluator = RegressionEvaluator(labelCol=\"Charges_incurred\",\n                              predictionCol=\"y_pred\",\n                              metricName=\"rmse\" )\n  r2_evaluator = RegressionEvaluator(labelCol=\"Charges_incurred\",\n                              predictionCol=\"y_pred\",\n                              metricName=\"r2\" )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f64ad67b-4f35-48f9-b803-60af3de7c113"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["perf_params = get_r2_rmse( lm, test_df )\n\n# create dataframe to store all the model performances\n\nimport pandas as pd\n\nmodel_perf = pd.DataFrame( columns = ['name', 'rsquared', 'rmse'] )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da7f7bc-cffb-4956-9de3-e53bb28d1d28"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["evaluator = RegressionEvaluator(\n  metricName=\"r2\",\n  labelCol=\"label\",\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd4c62fa-7fbc-43a2-8883-24ae680c1d4e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["crossval = CrossValidator(estimator=lrModel,\n                        estimatorParamMaps=paramGrid,\n                        evaluator=evaluator,\n                        numFolds=2)  # use 3+ folds in practice"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f588deb-6eef-475d-b322-c669c9f81646"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvModel = crossval.fit( train_df )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0f4ac1c-a746-4d49-9f99-494eb0b663ee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ridge_perf = get_r2_rmse( cvModel.bestModel, test_df )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3c2d383-cd45-4106-a137-db7861590fe6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model_perf = model_perf.append( pd.Series( [\"Ridge Regression\"] + ridge_perf ,\n                 index = model_perf.columns ),\n                 ignore_index = True )\n\nmodel_perf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bf36761-856e-4e52-8de3-279143c33a8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Using Lasso Regression  - Another method to avoid overfit \n# the regParam is a L1 (ridge) penalty, if elastic param is 1.0\nparamGrid = ParamGridBuilder()                          \\\n  .addGrid(lrModel.regParam, [0.1, 0.01, 0.001])      \\\n  .addGrid(lrModel.elasticNetParam, [1.0])            \\\n  .build()\n\nevaluator = RegressionEvaluator(\n  metricName=\"r2\",\n  labelCol=\"label\",\n)\n\ncrossval = CrossValidator(estimator=lrModel,\n                        estimatorParamMaps=paramGrid,\n                        evaluator=evaluator,\n                        numFolds=2)  # use 3+ folds in practice"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9c9c660-fbb1-47ce-80c1-8e94a7d967e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cvModel = crossval.fit( train_df )\nlasso_perf = get_r2_rmse( cvModel.bestModel, test_df )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5651d67-dd50-4ac8-aa85-b1f0bb1bd005"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model_perf = model_perf.append( pd.Series( [\"Lasso Regression\"] + lasso_perf ,\n                 index = model_perf.columns ),\n                 ignore_index = True )\n\nmodel_perf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"159f0efa-1ed9-4f1e-b206-ad8daf8dd2b9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e79c02d2-466b-4624-a3f1-4277adfa1f05"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"No.ofFaults_RevenuePredictor","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4321669744917240}},"nbformat":4,"nbformat_minor":0}
